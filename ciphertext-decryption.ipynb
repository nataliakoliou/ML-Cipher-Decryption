{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320e342b",
   "metadata": {},
   "source": [
    "# Machine-Learning Ciphertext Decryption Algorithm\n",
    "\n",
    "**Overview:** In this project, I implemented a machine-learning decryption algorithm that builds a multi-class classification model to decrypt ciphertexts, encrypted with some randomly generated mixed-ciphertext alphabet.\n",
    "\n",
    "## 1. Import all the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7333e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # provides regular expression matching operations\n",
    "import string\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import collections as col\n",
    "from sklearn.svm import SVC\n",
    "from collections import defaultdict as dd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366e94a",
   "metadata": {},
   "source": [
    "## 2. Implement manual feature extraction\n",
    "Identify and describe the most common features that define the internal structure of the dataset: since the datasets in this problem are text files, letters are the ones that form the core of this internal structure. I extracted therefore, features related with the way these letters behave inside words and sentences.\n",
    "\n",
    "**2.1. Single Letter Frequencies:** Calculate the number of times a single letter appears in the text and divide it by the total number of letters (NL). Then store this frequency value in a dictionary for later use (feature_FR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a9e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_FR(alphabet, letters, NL, accuracy):\n",
    "    feature_FR = dict.fromkeys(alphabet, 0)\n",
    "    for l in letters:\n",
    "        if l in alphabet:\n",
    "            feature_FR[l] += 1/NL\n",
    "    round_dict(feature_FR, accuracy)\n",
    "    return feature_FR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92c1fb",
   "metadata": {},
   "source": [
    "**2.2. Letter Occurencies in k-letter words:** Calculate the number of times a letter appears in a k-letter word (k=1,2,...) and divide it by the total number of its occurencies inside the text (letters_times). Then store this frequency value in a dictionary for later use (feature_WL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402ec1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_WL(alphabet, words, word_length, letters_times, accuracy):\n",
    "    feature_WL = dict.fromkeys(alphabet, 0)\n",
    "    if word_length == 1:\n",
    "        for w in words:\n",
    "            if w not in alphabet:\n",
    "                continue\n",
    "            elif len(w) == word_length and feature_WL[w] == 0:\n",
    "                feature_WL[w] = 1\n",
    "    elif has_domain(word_length, 2, 4):\n",
    "        for w in words:\n",
    "            if len(w) == word_length:\n",
    "                for l in list(w):\n",
    "                    if l not in alphabet:\n",
    "                        continue\n",
    "                    else:\n",
    "                        feature_WL[l] += 1/letters_times.get(l)\n",
    "    elif has_domain(word_length, 5, 7):\n",
    "        for w in words:\n",
    "            if has_domain(len(w), 5, 7):\n",
    "                for l in list(w):\n",
    "                    if l not in alphabet:\n",
    "                        continue\n",
    "                    else:\n",
    "                        feature_WL[l] += 1/letters_times.get(l)\n",
    "    elif has_domain(word_length, 8, 10):\n",
    "        for w in words:\n",
    "            if has_domain(len(w), 8, 10):\n",
    "                for l in list(w):\n",
    "                    if l not in alphabet:\n",
    "                        continue\n",
    "                    else:                    \n",
    "                        feature_WL[l] += 1/letters_times.get(l)\n",
    "    else:\n",
    "        for w in words:\n",
    "            if len(w) >= word_length:\n",
    "                for l in list(w):\n",
    "                    if l not in alphabet:\n",
    "                        continue\n",
    "                    else:                    \n",
    "                        feature_WL[l] += 1/letters_times.get(l)\n",
    "                    \n",
    "    round_dict(feature_WL, accuracy)\n",
    "    return feature_WL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d86a3",
   "metadata": {},
   "source": [
    "**2.3. Letter Position Frequencies:** Calculate the number of times a letter appears in the beginning/ending (or both) of words and divide it by the total number of its occurencies inside the text (letters_times). Then store this frequency value in a dictionary for later use (feature_SW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5b5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_SW(alphabet, case, words, letters_times, accuracy):\n",
    "    feature_SW = dict.fromkeys(alphabet, 0)\n",
    "    if \"first\" in case:\n",
    "        for w in words:\n",
    "            first = list(w)[0]\n",
    "            if first not in alphabet:\n",
    "                continue\n",
    "            else:\n",
    "                feature_SW[first] += 1/letters_times.get(first)\n",
    "    elif \"last\" in case:\n",
    "        for w in words:\n",
    "            last = list(w)[len(w)-1]\n",
    "            if last not in alphabet:\n",
    "                continue\n",
    "            else:            \n",
    "                feature_SW[last] += 1/letters_times.get(last)    \n",
    "    else:\n",
    "        for w in words:\n",
    "            first = list(w)[0]\n",
    "            last = list(w)[len(w)-1]\n",
    "            if first == last and first in alphabet:\n",
    "                feature_SW[first] += 1/letters_times.get(first)\n",
    "    round_dict(feature_SW, accuracy)\n",
    "    return feature_SW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8f3d3",
   "metadata": {},
   "source": [
    "**2.4. Double Letters Frequencies:** Calculate the number of times a double letter appears in the text and divide it by the total number of letters (NL). Then store this frequency value in a dictionary for later use (feature_DL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19c3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_DL(alphabet, words, letters_times, accuracy):\n",
    "    feature_DL = dict.fromkeys(alphabet, 0)\n",
    "    for w in words:\n",
    "        if len(w) != 1:\n",
    "            prev_letter = \"#\"\n",
    "            for l in list(w):\n",
    "                if prev_letter == l and l in alphabet:\n",
    "                    feature_DL[l] += 1/letters_times.get(l)\n",
    "                prev_letter = l\n",
    "    round_dict(feature_DL, accuracy)\n",
    "    return feature_DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397671d",
   "metadata": {},
   "source": [
    "## 3. Perform manual feature selection\n",
    "Create feature-set X and label-set y, by selecting the features that describe best each letter-class (A,B,...,Z): There are 11 available features in total. I use all of them by default, although users are encouraged to edit the features-tuple (feature_0, feature_1,...,feature_11), to observe the differencies in accuracy score estimations.\n",
    "\n",
    "Function *process* is called both in training and testing. Its input parameters though change!\n",
    "* In the training process the text-words list (super_words) is non-encrypted, whereas in the testing process it's decrypted with a randomly generated key (mixed ciphertext alphabet e.x: rcheyobdtmgiskuqlapfzjxnvw).\n",
    "* It's preferable to extract a good amount of samples from the training dataset in order to build a robust classification model. On that account, I chose to split the training dataset into 400 chunks. The testing dataset on the contrary remains as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0171f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(super_words, alphabet, chunks):\n",
    "    \n",
    "    accuracy = 10\n",
    "    features = []\n",
    "    labels = []\n",
    "    sub_words = list(divide_chunks(super_words, chunks))\n",
    "\n",
    "    for words in sub_words:\n",
    "        \n",
    "        letters = get_letters(words)\n",
    "        NL = len(letters)\n",
    "        letters_times = dict(sorted({key: value for key, value in dict(col.Counter(letters)).items()}.items()))\n",
    "\n",
    "        feature_0 = extract_feature_FR(alphabet, letters, NL, accuracy)\n",
    "        feature_1 = extract_feature_WL(alphabet, words, 1, letters_times, accuracy)\n",
    "        feature_2 = extract_feature_WL(alphabet, words, 2, letters_times, accuracy)\n",
    "        feature_3 = extract_feature_WL(alphabet, words, 3, letters_times, accuracy)\n",
    "        feature_4 = extract_feature_WL(alphabet, words, 4, letters_times, accuracy)\n",
    "        feature_5 = extract_feature_WL(alphabet, words, rand.randint(5,7), letters_times, accuracy)\n",
    "        feature_6 = extract_feature_WL(alphabet, words, rand.randint(8,10), letters_times, accuracy)\n",
    "        feature_7 = extract_feature_WL(alphabet, words, 11, letters_times, accuracy)\n",
    "        feature_8 = extract_feature_SW(alphabet, \"first\", words, letters_times, accuracy)\n",
    "        feature_9 = extract_feature_SW(alphabet, \"last\", words, letters_times, accuracy)\n",
    "        feature_10 = extract_feature_SW(alphabet, \"both\", words, letters_times, accuracy)\n",
    "        feature_11 = extract_feature_DL(alphabet, words, letters_times, accuracy)\n",
    "        \n",
    "        temp_features = dd(list) # defining an empty dictionary\n",
    "        for d in (feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9, feature_10, feature_11):\n",
    "            for key, value in d.items():\n",
    "                temp_features[key].append(value)\n",
    "        temp_features = dict(temp_features)\n",
    "        temp_features = [val for key, val in temp_features.items()]\n",
    "\n",
    "        features.extend(temp_features)\n",
    "        labels.extend(alphabet)\n",
    "        \n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65069374",
   "metadata": {},
   "source": [
    "## 4. Implement the classification model iteratively\n",
    "Train the classification model on a plaintext and assign labels (a,b,...,z) to the encrypted testing-letters: I begin by loading all the data that I'm going to use in the training and testing process (training_text, testing_text & dexryption_alphabet). By definition, the process of decryption requires a complete decription alphabet (y_pred) - and by complete I mean that *every letter of the *english alphabet, should appear exactly once in it*. No classification model, at least as far as I know, can guarantee that. It's actually pretty usual to get predictions with duplicate and missing classes e.x: [abaeyoddtagopkuklapezjpnve].\n",
    "\n",
    "To avoid this problem, I implemented an iterative classification process: I perform training and testing iteratively (updating the prediction set in the meantime), up until *y_pred* becomes a valid decryption alphabet set.\n",
    "\n",
    "**Attention:** Each iterative process maintains its non-duplicate results! In this way, I can ensure that all classification processes are somehow independent. Hence, the final classification accuracy score represents the cumulative accuracy that results from all these independent iterative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4c92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    training_text, testing_text, decryption_alphabet = load_local_data()\n",
    "\n",
    "    done = False\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    final_y = [\"NN\" for a in range(len(alphabet))]\n",
    "    np.set_printoptions(suppress=True)  # to avoid scientific notation when printing\n",
    "    \n",
    "    with open(training_text, 'r') as TR_f:\n",
    "        TR_words = TR_f.read().split()\n",
    "    with open(testing_text, 'r') as TE_f:\n",
    "        TE_words = TE_f.read().split()\n",
    "\n",
    "    while not done:\n",
    "        X_train, y_train = process(TR_words, alphabet, 400)\n",
    "        svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "\n",
    "        X_test, unused = process(TE_words, alphabet, len(list(TE_words)))\n",
    "        y_pred = svc.predict(X_test)\n",
    "        final_y = update_y(final_y, y_pred)\n",
    "        y_test = list(decryption_alphabet)\n",
    "\n",
    "        if is_shaffled_alphabet(y_pred):\n",
    "            done = True\n",
    "        else:\n",
    "            alphabet = update_alphabet(alphabet, final_y)\n",
    "            if len(alphabet) == 1:\n",
    "                final_y = update_y(final_y, alphabet)  # no prediction needed!\n",
    "                done = True\n",
    "  \n",
    "    accuracy = accuracy_score(y_test, final_y)\n",
    "    print(\"Accuracy Classification Score: {:.2f}\".format(accuracy))\n",
    "    print(\"-\"*127)\n",
    "\n",
    "    complete_alphabet = list(string.ascii_lowercase)\n",
    "    decrypt(testing_text, final_y, complete_alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08318e8d",
   "metadata": {},
   "source": [
    "## 5. Decrypt the testing ciphertext\n",
    "Apply the predicted decryption alphabet to the testing ciphertext: I replace each cipher-letter with its corresponding decrypted one. This substitution is based on the final predicted decryption alphabet (final_y).\n",
    "\n",
    "I store the decrypted testing text locally and then display it as an output to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c976740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt(text, fy, alphabet):\n",
    "\n",
    "    decr_dict = {}\n",
    "    for i in range(len(alphabet)):\n",
    "        decr_dict[alphabet[i]] = fy[i]\n",
    "\n",
    "    with open(text, 'r') as f:\n",
    "        encr_text = f.read()\n",
    "        re.split(r'(\\s+)', encr_text)\n",
    "        decr_text = \"\"\n",
    "        for count, encr_char in enumerate(get_letters(encr_text)):\n",
    "            if encr_char.isspace():\n",
    "                decr_text = decr_text + encr_char\n",
    "            elif encr_char not in decr_dict:\n",
    "                decr_text = decr_text + encr_char\n",
    "            else:\n",
    "                decr_text = decr_text + decr_dict[encr_char]\n",
    "\n",
    "    with open(\"output.txt\", 'w') as output:\n",
    "        output.write(decr_text)\n",
    "    with open(\"output.txt\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        print(\"Decrypted Ciphertext: \\n\")\n",
    "        print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a4904",
   "metadata": {},
   "source": [
    "## 6. Integrate all the necessary interprocedural functions\n",
    "These are auxiliary functions that simplify both the classification and the decryption process.\n",
    "\n",
    "**6.1. has_domain:** checks whether some initialized variable (var) is arithmetically bounded by two fixed points (point1 & point2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093a4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_domain(var, point1, point2):\n",
    "    if var >= point1 and var <= point2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f43ac",
   "metadata": {},
   "source": [
    "**6.2. round_dict:** rounds the values of some dictionary's keys (dict), according to a fixed (accuracy) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a1a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_dict(dict, accuracy):\n",
    "    for key in dict: dict[key] = round(dict.get(key),accuracy)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cf60c",
   "metadata": {},
   "source": [
    "**6.3. get_letters:** returns a list of all the letters included in a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca438e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letters(words):\n",
    "    temp = []\n",
    "    for w in words:\n",
    "        temp.append(list(w))\n",
    "    letters = [letter for word in temp for letter in word]\n",
    "    return letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a8f10",
   "metadata": {},
   "source": [
    "**6.4. divide_chunks:** divides a *list* of items into n chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32505eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_chunks(list, n):\n",
    "    for i in range(0, len(list), n):\n",
    "        yield list[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8c61f",
   "metadata": {},
   "source": [
    "**6.5. is_shaffled_alphabet:** checks whether the alphabet list (key list or y_pred as we know it) used in the classification process, contains any duplicate or missing letters.\n",
    "\n",
    "Reminder: alphabet gets shorter over time, as iterative classifications take place. Check out 6.6 to see why **:)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_shaffled_alphabet(key):\n",
    "    if len(key) == len(set(key)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe81f4",
   "metadata": {},
   "source": [
    "**6.6. update_y:** updates the predicted decryption alphabet after each classification.\n",
    "\n",
    "In the very beginning, I initialized the decryption alphabet list with unknown values NN, since I knew nothing about it at that time - before any prediction takes place. Every time I run the classification process, I get some decryption alphabet (y or y_pred as we know it). If this alphabet contains duplicates/missing letters (invalid), these duplicates need to be removed (replaced with NN), so that they can available for re-prediction in some future iteration of the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_y(fy, y):\n",
    "    count = 0\n",
    "    for i in range(len(fy)):\n",
    "        if fy[i] == \"NN\":\n",
    "            if y[count] not in fy:\n",
    "                fy[i] = y[count]\n",
    "            count += 1\n",
    "        else:\n",
    "            continue        \n",
    "    return fy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb106e91",
   "metadata": {},
   "source": [
    "**6.7. update_alphabet:** updates the alphabet list that is used to build y_train and y_test, after each classification.\n",
    "\n",
    "The training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c19381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alphabet(alphabet, fy):\n",
    "    new_AB = []\n",
    "    for val in alphabet:\n",
    "        if val not in fy:\n",
    "            new_AB.append(val)\n",
    "    return new_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82e288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57e14cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_data():\n",
    "\n",
    "    training_text = \"TRAINING-tolstoy-anna-karenina.txt\"\n",
    "\n",
    "    testing_text_1 = \"TESTING-tolstoy-anna-karenina-1.txt\"\n",
    "    decryption_alphabet_1 = \"rgbhdtkclvnqjxfspamioyzweu\"  # encryption_alphabet_1 = \"rcheyobdtmgiskuqlapfzjxnvw\"\n",
    "\n",
    "    return training_text, testing_text_1, decryption_alphabet_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
